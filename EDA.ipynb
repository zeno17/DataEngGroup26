{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import correlationfunctions as cr\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "print(sys.version) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('spark count')\n",
    "sc = SparkContext(conf = conf)\n",
    "data = ['data engineering', 'on tuesday']\n",
    "datardd = sc.parallelize(data)\n",
    "result = datardd.map(lambda x:x.split(' ')).collect()\n",
    "result1 = datardd.flatMap(lambda x:x.split(' ')).collect()\n",
    "print(result)\n",
    "print(result1)\n",
    "\n",
    "#4. Websites that I used for this installation:\n",
    "#https://www.knowledgehut.com/blog/big-data/how-to-install-apache-spark-on-windows\n",
    "#https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('C:/Users/Zeno/Google Drive/Data science in Engineering/2IMD15 Data Engineering/data/')\n",
    "filenames = [file for file in files if file[-4:] == '.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['202001_Amsterdam_AALB_NoExpiry.txt',\n",
       " '202001_Amsterdam_AGN_NoExpiry.txt',\n",
       " '202001_Amsterdam_AKZA_NoExpiry.txt',\n",
       " '202001_Amsterdam_APAM_NoExpiry.txt',\n",
       " '202001_Amsterdam_ASML_NoExpiry.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different version of correlation computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_greg(in1, in2):\n",
    "    # Fully custom function\n",
    "    in1 = in1.array\n",
    "    in2 = in2.array\n",
    "    in1_avg = get_average(in1)\n",
    "    in2_avg = get_average(in2)\n",
    "\n",
    "    topsum = 0\n",
    "    blssum = 0\n",
    "    brssum = 0\n",
    "    for x in range(len(in1)):\n",
    "        topsum += (in1[x] - in1_avg) * (in2[x] - in2_avg)\n",
    "        blssum += (in1[x] - in1_avg)**2\n",
    "        brssum += (in2[x] - in2_avg)**2\n",
    "\n",
    "    return topsum / (sqrt(blssum) * sqrt(brssum))\n",
    "\n",
    "def get_average(dataset):\n",
    "    # Custom average function\n",
    "    average = 0\n",
    "    for x in range(len(dataset)):\n",
    "        average += dataset[x]\n",
    "    return average / len(dataset)\n",
    "\n",
    "def greg_spearman(x, y):\n",
    "    sum = 0\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    n = len(x)\n",
    "    rank1 = rank(x)\n",
    "    rank2 = rank(y)\n",
    "\n",
    "    for i in range (n):\n",
    "        sum += (rank1[i] - rank2[i])**2\n",
    "\n",
    "    return 1 - (6*sum)/(n*(n*n-1))\n",
    "\n",
    "\n",
    "# Computes ranks of an array a without accounting for duplicates\n",
    "def rank_noduplicates(a):\n",
    "    return sorted(range(len(a)), key=a.__getitem__)\n",
    "\n",
    "\n",
    "# Computes ranks of an array a and accounts for duplicates\n",
    "def rank(a):\n",
    "    n = len(a)\n",
    "    indeces = rank_noduplicates(a)\n",
    "    sorted_values = [a[index] for index in indeces]\n",
    "    duplicates = 0 # number of elements with the same value (1 if no duplicates)\n",
    "    sum_ranks = 0 # sum of ranks of elements\n",
    "    ranked = [0]*n # the ranks of the array a\n",
    "    \n",
    "    for i in range(n):\n",
    "        duplicates += 1\n",
    "        sum_ranks += i\n",
    "\n",
    "        # If there are no more elements or no more elements with equal values\n",
    "        if i == n-1 or sorted_values[i] != sorted_values[i+1]:\n",
    "            # Set rank of elements to the average rank\n",
    "            average_rank = sum_ranks / float(duplicates) + 1\n",
    "            for j in range(i - duplicates + 1, i + 1):\n",
    "                ranked[indeces[j]] = average_rank        \n",
    "            \n",
    "            # Clear the counters for the next iteration\n",
    "            duplicates = 0\n",
    "            sum_ranks = 0\n",
    "\n",
    "    return ranked \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-09 22:06:32.175450\n",
      "scipy_pearson 0:00:00 [(0.477017157117237, 7.433192538852585e-296)]\n",
      "custom_pearson 0:00:00 [0.477017157117237]\n",
      "pearson_greg 0:00:00.077791 [0.47701715711723813]\n",
      "scipy_spearman 0:00:00.001995 [SpearmanrResult(correlation=0.39989977080969313, pvalue=2.236656602971081e-200)]\n",
      "custom_spearman 0:00:00.000997 [0.399899770809693]\n",
      "greg_spearman 0:00:00.009972 [0.3999277514127969]\n",
      "2020-05-09 22:06:33.939709\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def correlation(series1, series2, metric='opening', \n",
    "                match='omit',duplicates=True, \n",
    "                correlation='pearson',showplot=False):\n",
    "    '''\n",
    "    series1: a pandas DataFrame\n",
    "    series2: a pandas DataFrame\n",
    "    metric: What metric in the data to use (selected from ['date','time','opening','highest','lower','closing','volume'])\n",
    "    match: how to deal with misalignment of the timeseries\n",
    "    duplicates: what to do if there are multiple occurences of a date_time in 1 dataframe\n",
    "    correlation: What correlation coefficient to use\n",
    "    '''   \n",
    "    #only keep column you need\n",
    "    series1 = series1[['date_time',metric]]\n",
    "    series2 = series2[['date_time',metric]]\n",
    "    \n",
    "    #print(series1.head())\n",
    "    #print(series2.head())\n",
    "    series1_mask = series1['date_time'].isin(series2['date_time'])\n",
    "    series2_mask = series2['date_time'].isin(series1['date_time'])\n",
    "    \n",
    "    #print(series1_mask.head())\n",
    "    #print(series2_mask.head())\n",
    "    \n",
    "    if match=='omit':\n",
    "        # only take data that is present in both series\n",
    "        filt_series1 = series1[series1_mask]\n",
    "        filt_series2 = series2[series2_mask]\n",
    "    \n",
    "    if duplicates==True:\n",
    "        filt_series1 = filt_series1.drop_duplicates(subset='date_time')\n",
    "        filt_series2 = filt_series2.drop_duplicates(subset='date_time')\n",
    "    if showplot==True:\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        plt.plot('date_time', metric, '--r',data=series1)\n",
    "        plt.plot('date_time', metric, '--b',data=series2)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "    \n",
    "    corr_function = None\n",
    "    if correlation =='scipy_pearson':\n",
    "        corr_function = pearsonr\n",
    "    elif correlation == 'custom_pearson':\n",
    "        corr_function = cr.pearsonrcustom\n",
    "    elif correlation == 'pearson_greg':\n",
    "        corr_function = pearson_greg\n",
    "    elif correlation == 'scipy_spearman':\n",
    "        corr_function = spearmanr\n",
    "    elif correlation == 'custom_spearman':\n",
    "        corr_function = cr.spearmanrcustom\n",
    "    elif correlation == 'greg_spearman':\n",
    "        corr_function = greg_spearman\n",
    "    start_time = datetime.now()    \n",
    "    correlations = [corr_function(filt_series1[metric], filt_series2[metric])]\n",
    "    end_time = datetime.now()\n",
    "    print(correlation, end_time-start_time, correlations)\n",
    "    return correlations\n",
    "               \n",
    "print(datetime.now())\n",
    "test1 = pd.read_csv('C:/Users/Zeno/Google Drive/Data science in Engineering/2IMD15 Data Engineering/data/' + filenames[0], sep=\",\", header=None, \n",
    "                                      names=['date','time','opening','highest','lower','closing','volume'],\n",
    "                                      parse_dates=[['date', 'time']])\n",
    "test2 = pd.read_csv('C:/Users/Zeno/Google Drive/Data science in Engineering/2IMD15 Data Engineering/data/' + filenames[1], sep=\",\", header=None,\n",
    "                      names=['date','time','opening','highest','lower','closing','volume'],\n",
    "                      parse_dates=[['date', 'time']])\n",
    "\n",
    "correlation(test1,test2,correlation='scipy_pearson')\n",
    "correlation(test1,test2,correlation='custom_pearson')\n",
    "correlation(test1,test2,correlation='pearson_greg')\n",
    "\n",
    "correlation(test1,test2,correlation='scipy_spearman')\n",
    "correlation(test1,test2,correlation='custom_spearman')\n",
    "correlation(test1,test2,correlation='greg_spearman')\n",
    "\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-2b05a233226b>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-2b05a233226b>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    pearson[i][j] = = correlation(series1, series2)\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def multi_corr(filelist, showplot=True):\n",
    "    '''\n",
    "    filelist: a list with filenames that it needs to analyze and cross reference on correlations.\n",
    "    '''\n",
    "    #initialize scores\n",
    "    pearson = np.ones(shape=(len(filelist), len(filelist)))\n",
    "    spearman = np.ones(shape=(len(filelist), len(filelist)))\n",
    "    stocknames = [name.split('_')[2] for name in filelist]\n",
    "    \n",
    "    for i, name1 in enumerate(filelist):\n",
    "        for j, name2 in enumerate(filelist):\n",
    "            if name1 != name2:\n",
    "                # TODO\n",
    "                # Turn it into being compatible with spark, e.g.:\n",
    "                # df = spark.read.csv(\"src/main/resources/zipcodes.csv\")\n",
    "                \n",
    "                series1 = pd.read_csv('C:/Users/Zeno/Google Drive/Data science in Engineering/2IMD15 Data Engineering/data/' + filelist[i], sep=\",\", header=None, \n",
    "                                      names=['date','time','opening','highest','lower','closing','volume'],\n",
    "                                      parse_dates=[['date', 'time']])\n",
    "                series2 = pd.read_csv('C:/Users/Zeno/Google Drive/Data science in Engineering/2IMD15 Data Engineering/data/' + filelist[j], sep=\",\", header=None,\n",
    "                                      names=['date','time','opening','highest','lower','closing','volume'],\n",
    "                                      parse_dates=[['date', 'time']])\n",
    "                \n",
    "                pearson[i][j] = = correlation(series1, series2)\n",
    "                \n",
    "                #add values to initialized array\n",
    "                #only interested in correlation, not in corresponding p value\n",
    "                pearson[i][j] = corr[0][0]\n",
    "                spearman[i][j] = corr[0][0]\n",
    "    \n",
    "    text_size =  20\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(20,8))\n",
    "    sns.heatmap(pearson, annot=True, ax=ax[0], annot_kws={\"size\":text_size})\n",
    "    #im = ax[0].imshow(pearson)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    \n",
    "    #ax[0].set_xticks(np.arange(len(stocknames)))\n",
    "    #ax[0].set_yticks(np.arange(len(stocknames)))\n",
    "    ax[0].tick_params(axis='both', labelsize=text_size)\n",
    "    ax[0].set_xticklabels(stocknames)\n",
    "    ax[0].set_yticklabels(stocknames)\n",
    "    ax[0].set_title(\"Pearson\", fontsize=text_size)\n",
    "    \n",
    "    #im = ax[1].imshow(spearman)\n",
    "    sns.heatmap(pearson, annot=True, ax=ax[1], annot_kws={\"size\":text_size})\n",
    "    ax[1].tick_params(axis='both', labelsize=text_size)\n",
    "    #ax[1].set_xticks(np.arange(len(stocknames)))\n",
    "    #ax[1].set_yticks(np.arange(len(stocknames)))\n",
    "    ax[1].set_xticklabels(stocknames)\n",
    "    ax[1].set_yticklabels(stocknames)\n",
    "    ax[1].set_title(\"Spearman\", fontsize=text_size)      \n",
    "\n",
    "    \n",
    "    \n",
    "bankfiles = ['202001_NYSE-American_BHB_NoExpiry.txt',\n",
    "             '202001_NYSE_BAC_NoExpiry.txt',\n",
    "             '202001_NYSE_HDB_NoExpiry.txt',\n",
    "             '202001_NYSE_IBN_NoExpiry.txt']\n",
    "\n",
    "print(\"Start: \",datetime.now())\n",
    "multi_corr(bankfiles)\n",
    "print(\"End: \", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import intersect1d\n",
    "\n",
    "def prep_for_corr(input1,input2):\n",
    "    '''\n",
    "    input1 = [[<dates], [<values>]]\n",
    "    input2 = [[<dates], [<values>]]\n",
    "    Computes data based on values for which the date is present in both inputs\n",
    "    '''\n",
    "    \n",
    "    matches, ind_inp1, ind_inp2, matching_dates = np.intersect1d(input1[0], input2[0], return_indices=True)\n",
    "    series1 = np.array(input1[1])[ind_inp1]\n",
    "    series2 = np.array(input2[1])[ind_inp2]\n",
    "    return series1, series2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,1,2,3,4,5])\n",
    "b = np.array([1,2,3,4,5,6])\n",
    "\n",
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-09 16:44:06.754139\n",
      "2020-05-09 16:44:06.766109\n",
      "2020-05-09 16:44:06.771095\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "b = []\n",
    "\n",
    "print(datetime.now())\n",
    "for i in range(0,100000):\n",
    "    a.append(i)\n",
    "print(datetime.now())\n",
    "b = [i for i in range(0,100000)]\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.57142857 -0.57142857 -0.57142857  0.42857143  0.42857143  0.42857143\n",
      "  0.42857143] [-0.57142857  0.42857143  1.42857143  2.42857143  3.42857143  4.42857143\n",
      "  5.42857143] 1.3093073414159542 8.323804075404123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5505386303477302, 0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0, 0, 0, 1, 1, 1, 1])\n",
    "b = np.arange(7)\n",
    "pearsonrcustom(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.  0.  1.  2.] [ 7.   6.  -0.5  3.   1. ] 3.1622776601683795 9.759610647971568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.48602517675625084, 0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "b = [10, 9, 2.5, 6, 4]\n",
    "pearsonrcustom(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
